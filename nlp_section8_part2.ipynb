{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sequence-to-Sequence Model with LSTM and Attention\n",
                "\n",
                "## Introduction\n",
                "\n",
                "In this notebook, we'll build a **Sequence-to-Sequence (Seq2Seq)** model for English-French translation.\n",
                "\n",
                "### Key Components:\n",
                "1. **Encoder**: LSTM that processes the input sequence (English)\n",
                "2. **Attention Mechanism**: Allows the decoder to focus on different parts of the input\n",
                "3. **Decoder**: LSTM that generates the output sequence (French)\n",
                "\n",
                "### Dataset:\n",
                "- English-French sentence pairs from `data/eng_french.csv`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cpu\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from collections import Counter\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "torch.manual_seed(42)\n",
                "np.random.seed(42)\n",
                "\n",
                "# Device configuration\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load and Prepare Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset shape: (175621, 2)\n",
                        "\n",
                        "First few rows:\n",
                        "  English words/sentences French words/sentences\n",
                        "0                     Hi.                 Salut!\n",
                        "1                    Run!                Cours !\n",
                        "2                    Run!               Courez !\n",
                        "3                    Who?                  Qui ?\n",
                        "4                    Wow!             Ça alors !\n",
                        "\n",
                        "Using 50000 sentence pairs\n"
                    ]
                }
            ],
            "source": [
                "# Load the dataset\n",
                "df = pd.read_csv('data/eng_french.csv')\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"\\nFirst few rows:\")\n",
                "print(df.head())\n",
                "\n",
                "# Take a subset for faster training (can be adjusted)\n",
                "df = df.sample(n=min(50000, len(df)), random_state=42).reset_index(drop=True)\n",
                "print(f\"\\nUsing {len(df)} sentence pairs\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sample preprocessed data:\n",
                        "                                       eng_clean  \\\n",
                        "0                                  take a seat .   \n",
                        "1                          i wish tom was here .   \n",
                        "2                      how did the audition go ?   \n",
                        "3  i've no friend to talk to about my problems .   \n",
                        "4   i really like this skirt . can i try it on ?   \n",
                        "\n",
                        "                                            fr_clean  \n",
                        "0                                     prends place !  \n",
                        "1                       j'aimerais que tom soit là .  \n",
                        "2                 comment s'est passée l'audition  ?  \n",
                        "3  je n'ai pas d'ami avec lequel je puisse m'entr...  \n",
                        "4  j'aime beaucoup cette jupe , puis-je l'essayer  ?  \n"
                    ]
                }
            ],
            "source": [
                "import re\n",
                "\n",
                "# Special tokens\n",
                "SOS_TOKEN = '<SOS>'\n",
                "EOS_TOKEN = '<EOS>'\n",
                "PAD_TOKEN = '<PAD>'\n",
                "UNK_TOKEN = '<UNK>'\n",
                "\n",
                "def preprocess_sentence(sentence):\n",
                "    \"\"\"Basic preprocessing: lowercase and clean punctuation\"\"\"\n",
                "    sentence = sentence.lower().strip()\n",
                "    # Add space before punctuation\n",
                "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
                "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
                "    sentence = sentence.strip()\n",
                "    return sentence\n",
                "\n",
                "# Preprocess sentences\n",
                "df['eng_clean'] = df['English words/sentences'].apply(preprocess_sentence)\n",
                "df['fr_clean'] = df['French words/sentences'].apply(preprocess_sentence)\n",
                "\n",
                "print(\"Sample preprocessed data:\")\n",
                "print(df[['eng_clean', 'fr_clean']].head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Build Vocabulary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "English vocabulary size: 9581\n",
                        "French vocabulary size: 17309\n"
                    ]
                }
            ],
            "source": [
                "class Vocabulary:\n",
                "    def __init__(self, name):\n",
                "        self.name = name\n",
                "        self.word2index = {PAD_TOKEN: 0, SOS_TOKEN: 1, EOS_TOKEN: 2, UNK_TOKEN: 3}\n",
                "        self.index2word = {0: PAD_TOKEN, 1: SOS_TOKEN, 2: EOS_TOKEN, 3: UNK_TOKEN}\n",
                "        self.word_count = {}\n",
                "        self.n_words = 4  # Count PAD, SOS, EOS, UNK\n",
                "    \n",
                "    def add_sentence(self, sentence):\n",
                "        for word in sentence.split():\n",
                "            self.add_word(word)\n",
                "    \n",
                "    def add_word(self, word):\n",
                "        if word not in self.word2index:\n",
                "            self.word2index[word] = self.n_words\n",
                "            self.index2word[self.n_words] = word\n",
                "            self.word_count[word] = 1\n",
                "            self.n_words += 1\n",
                "        else:\n",
                "            self.word_count[word] += 1\n",
                "\n",
                "# Build vocabularies\n",
                "eng_vocab = Vocabulary('english')\n",
                "fr_vocab = Vocabulary('french')\n",
                "\n",
                "for _, row in df.iterrows():\n",
                "    eng_vocab.add_sentence(row['eng_clean'])\n",
                "    fr_vocab.add_sentence(row['fr_clean'])\n",
                "\n",
                "print(f\"English vocabulary size: {eng_vocab.n_words}\")\n",
                "print(f\"French vocabulary size: {fr_vocab.n_words}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dataset Class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training samples: 40000\n",
                        "Validation samples: 10000\n",
                        "Batches per epoch: 625\n"
                    ]
                }
            ],
            "source": [
                "class TranslationDataset(Dataset):\n",
                "    def __init__(self, df, eng_vocab, fr_vocab, max_len=50):\n",
                "        self.df = df\n",
                "        self.eng_vocab = eng_vocab\n",
                "        self.fr_vocab = fr_vocab\n",
                "        self.max_len = max_len\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "    \n",
                "    def sentence_to_indices(self, sentence, vocab, add_sos_eos=False):\n",
                "        indices = []\n",
                "        if add_sos_eos:\n",
                "            indices.append(vocab.word2index[SOS_TOKEN])\n",
                "        \n",
                "        for word in sentence.split():\n",
                "            if word in vocab.word2index:\n",
                "                indices.append(vocab.word2index[word])\n",
                "            else:\n",
                "                indices.append(vocab.word2index[UNK_TOKEN])\n",
                "        \n",
                "        if add_sos_eos:\n",
                "            indices.append(vocab.word2index[EOS_TOKEN])\n",
                "        \n",
                "        # Truncate if too long\n",
                "        indices = indices[:self.max_len]\n",
                "        \n",
                "        # Pad if too short\n",
                "        while len(indices) < self.max_len:\n",
                "            indices.append(vocab.word2index[PAD_TOKEN])\n",
                "        \n",
                "        return indices\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        \n",
                "        # Encoder input (English) - no SOS/EOS\n",
                "        eng_indices = self.sentence_to_indices(row['eng_clean'], self.eng_vocab, add_sos_eos=False)\n",
                "        \n",
                "        # Decoder input (French) - with SOS/EOS\n",
                "        fr_indices = self.sentence_to_indices(row['fr_clean'], self.fr_vocab, add_sos_eos=True)\n",
                "        \n",
                "        return torch.tensor(eng_indices, dtype=torch.long), torch.tensor(fr_indices, dtype=torch.long)\n",
                "\n",
                "# Split data\n",
                "train_size = int(0.8 * len(df))\n",
                "train_df = df[:train_size]\n",
                "val_df = df[train_size:]\n",
                "\n",
                "# Create datasets\n",
                "train_dataset = TranslationDataset(train_df, eng_vocab, fr_vocab)\n",
                "val_dataset = TranslationDataset(val_df, eng_vocab, fr_vocab)\n",
                "\n",
                "# Create dataloaders\n",
                "batch_size = 64\n",
                "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
                "\n",
                "print(f\"Training samples: {len(train_dataset)}\")\n",
                "print(f\"Validation samples: {len(val_dataset)}\")\n",
                "print(f\"Batches per epoch: {len(train_loader)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Attention Mechanism\n",
                "\n",
                "### Bahdanau Attention (Additive Attention)\n",
                "\n",
                "The attention mechanism computes a weighted sum of encoder hidden states:\n",
                "\n",
                "```\n",
                "score(h_t, h_s) = v^T tanh(W_1 h_t + W_2 h_s)\n",
                "attention_weights = softmax(scores)\n",
                "context = sum(attention_weights * encoder_outputs)\n",
                "```\n",
                "\n",
                "Where:\n",
                "- h_t: current decoder hidden state\n",
                "- h_s: encoder hidden state at position s\n",
                "- v, W_1, W_2: learned parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Attention(nn.Module):\n",
                "    \"\"\"\n",
                "    Bahdanau (Additive) Attention mechanism.\n",
                "    \"\"\"\n",
                "    def __init__(self, hidden_dim):\n",
                "        super(Attention, self).__init__()\n",
                "        self.hidden_dim = hidden_dim\n",
                "        \n",
                "        # W_1: transforms encoder outputs\n",
                "        self.W1 = nn.Linear(hidden_dim, hidden_dim)\n",
                "        # W_2: transforms decoder hidden state\n",
                "        self.W2 = nn.Linear(hidden_dim, hidden_dim)\n",
                "        # v: combines the transformed states\n",
                "        self.V = nn.Linear(hidden_dim, 1)\n",
                "    \n",
                "    def forward(self, decoder_hidden, encoder_outputs):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            decoder_hidden: (batch_size, hidden_dim)\n",
                "            encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
                "        \n",
                "        Returns:\n",
                "            context: (batch_size, hidden_dim)\n",
                "            attention_weights: (batch_size, seq_len)\n",
                "        \"\"\"\n",
                "        batch_size = encoder_outputs.size(0)\n",
                "        seq_len = encoder_outputs.size(1)\n",
                "        \n",
                "        # Repeat decoder hidden state seq_len times\n",
                "        # (batch_size, seq_len, hidden_dim)\n",
                "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
                "        \n",
                "        # Calculate attention scores\n",
                "        # energy shape: (batch_size, seq_len, hidden_dim)\n",
                "        energy = torch.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden))\n",
                "        \n",
                "        # attention shape: (batch_size, seq_len)\n",
                "        scores = self.V(energy).squeeze(2)\n",
                "        \n",
                "        # Get attention weights (batch_size, seq_len)\n",
                "        attention_weights = torch.softmax(scores, dim=1)\n",
                "        \n",
                "        # Calculate context vector\n",
                "        # (batch_size, hidden_dim)\n",
                "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
                "        \n",
                "        return context, attention_weights"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Encoder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Encoder(nn.Module):\n",
                "    \"\"\"\n",
                "    LSTM Encoder for the input sequence.\n",
                "    \"\"\"\n",
                "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.5):\n",
                "        super(Encoder, self).__init__()\n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.num_layers = num_layers\n",
                "        \n",
                "        # Embedding layer\n",
                "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
                "        \n",
                "        # LSTM layer\n",
                "        self.lstm = nn.LSTM(\n",
                "            embedding_dim,\n",
                "            hidden_dim,\n",
                "            num_layers=num_layers,\n",
                "            batch_first=True,\n",
                "            dropout=dropout if num_layers > 1 else 0\n",
                "        )\n",
                "        \n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            x: (batch_size, seq_len)\n",
                "        \n",
                "        Returns:\n",
                "            outputs: (batch_size, seq_len, hidden_dim)\n",
                "            hidden: (num_layers, batch_size, hidden_dim)\n",
                "            cell: (num_layers, batch_size, hidden_dim)\n",
                "        \"\"\"\n",
                "        # Embedding: (batch_size, seq_len, embedding_dim)\n",
                "        embedded = self.dropout(self.embedding(x))\n",
                "        \n",
                "        # LSTM outputs\n",
                "        outputs, (hidden, cell) = self.lstm(embedded)\n",
                "        \n",
                "        return outputs, hidden, cell"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Decoder with Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Decoder(nn.Module):\n",
                "    \"\"\"\n",
                "    LSTM Decoder with Attention mechanism.\n",
                "    \"\"\"\n",
                "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.5):\n",
                "        super(Decoder, self).__init__()\n",
                "        self.vocab_size = vocab_size\n",
                "        self.hidden_dim = hidden_dim\n",
                "        self.num_layers = num_layers\n",
                "        \n",
                "        # Embedding layer\n",
                "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
                "        \n",
                "        # Attention mechanism\n",
                "        self.attention = Attention(hidden_dim)\n",
                "        \n",
                "        # LSTM layer (input: embedding + context)\n",
                "        self.lstm = nn.LSTM(\n",
                "            embedding_dim + hidden_dim,  # Embedding + context vector\n",
                "            hidden_dim,\n",
                "            num_layers=num_layers,\n",
                "            batch_first=True,\n",
                "            dropout=dropout if num_layers > 1 else 0\n",
                "        )\n",
                "        \n",
                "        # Output layer\n",
                "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
                "        \n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "    \n",
                "    def forward(self, x, hidden, cell, encoder_outputs):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            x: (batch_size, 1) - single time step\n",
                "            hidden: (num_layers, batch_size, hidden_dim)\n",
                "            cell: (num_layers, batch_size, hidden_dim)\n",
                "            encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
                "        \n",
                "        Returns:\n",
                "            output: (batch_size, vocab_size)\n",
                "            hidden: (num_layers, batch_size, hidden_dim)\n",
                "            cell: (num_layers, batch_size, hidden_dim)\n",
                "            attention_weights: (batch_size, seq_len)\n",
                "        \"\"\"\n",
                "        # Embedding: (batch_size, 1, embedding_dim)\n",
                "        embedded = self.dropout(self.embedding(x))\n",
                "        \n",
                "        # Calculate attention using the last layer's hidden state\n",
                "        # hidden[-1]: (batch_size, hidden_dim)\n",
                "        context, attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
                "        \n",
                "        # Concatenate embedding and context: (batch_size, 1, embedding_dim + hidden_dim)\n",
                "        lstm_input = torch.cat([embedded, context.unsqueeze(1)], dim=2)\n",
                "        \n",
                "        # LSTM forward\n",
                "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
                "        \n",
                "        # Output: (batch_size, vocab_size)\n",
                "        output = self.fc(output.squeeze(1))\n",
                "        \n",
                "        return output, hidden, cell, attention_weights"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Seq2Seq Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seq2Seq Model Architecture:\n",
                        "\n",
                        "Encoder parameters: 6,130,944\n",
                        "Decoder parameters: 18,563,230\n",
                        "Total parameters: 24,694,174\n"
                    ]
                }
            ],
            "source": [
                "class Seq2Seq(nn.Module):\n",
                "    \"\"\"\n",
                "    Complete Sequence-to-Sequence model with attention.\n",
                "    \"\"\"\n",
                "    def __init__(self, encoder, decoder, device):\n",
                "        super(Seq2Seq, self).__init__()\n",
                "        self.encoder = encoder\n",
                "        self.decoder = decoder\n",
                "        self.device = device\n",
                "    \n",
                "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            src: source sequence (batch_size, src_len)\n",
                "            trg: target sequence (batch_size, trg_len)\n",
                "            teacher_forcing_ratio: probability of using teacher forcing\n",
                "        \n",
                "        Returns:\n",
                "            outputs: (batch_size, trg_len, vocab_size)\n",
                "        \"\"\"\n",
                "        batch_size = src.size(0)\n",
                "        trg_len = trg.size(1)\n",
                "        trg_vocab_size = self.decoder.vocab_size\n",
                "        \n",
                "        # Tensor to store decoder outputs\n",
                "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
                "        \n",
                "        # Encode the source sequence\n",
                "        encoder_outputs, hidden, cell = self.encoder(src)\n",
                "        \n",
                "        # First input to decoder is <SOS> token\n",
                "        decoder_input = trg[:, 0].unsqueeze(1)  # (batch_size, 1)\n",
                "        \n",
                "        # Decode one token at a time\n",
                "        for t in range(1, trg_len):\n",
                "            # Forward through decoder\n",
                "            output, hidden, cell, attention_weights = self.decoder(\n",
                "                decoder_input, hidden, cell, encoder_outputs\n",
                "            )\n",
                "            \n",
                "            # Store output\n",
                "            outputs[:, t, :] = output\n",
                "            \n",
                "            # Teacher forcing: use ground truth as next input\n",
                "            if np.random.random() < teacher_forcing_ratio:\n",
                "                decoder_input = trg[:, t].unsqueeze(1)\n",
                "            else:\n",
                "                # Use model's own prediction\n",
                "                decoder_input = output.argmax(1).unsqueeze(1)\n",
                "        \n",
                "        return outputs\n",
                "\n",
                "# Model hyperparameters\n",
                "EMBEDDING_DIM = 256\n",
                "HIDDEN_DIM = 512\n",
                "NUM_LAYERS = 2\n",
                "DROPOUT = 0.5\n",
                "\n",
                "# Initialize model\n",
                "encoder = Encoder(\n",
                "    vocab_size=eng_vocab.n_words,\n",
                "    embedding_dim=EMBEDDING_DIM,\n",
                "    hidden_dim=HIDDEN_DIM,\n",
                "    num_layers=NUM_LAYERS,\n",
                "    dropout=DROPOUT\n",
                ").to(device)\n",
                "\n",
                "decoder = Decoder(\n",
                "    vocab_size=fr_vocab.n_words,\n",
                "    embedding_dim=EMBEDDING_DIM,\n",
                "    hidden_dim=HIDDEN_DIM,\n",
                "    num_layers=NUM_LAYERS,\n",
                "    dropout=DROPOUT\n",
                ").to(device)\n",
                "\n",
                "model = Seq2Seq(encoder, decoder, device).to(device)\n",
                "\n",
                "print(\"Seq2Seq Model Architecture:\")\n",
                "print(f\"\\nEncoder parameters: {sum(p.numel() for p in encoder.parameters()):,}\")\n",
                "print(f\"Decoder parameters: {sum(p.numel() for p in decoder.parameters()):,}\")\n",
                "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(model, dataloader, criterion, optimizer, clip, teacher_forcing_ratio):\n",
                "    \"\"\"Train for one epoch\"\"\"\n",
                "    model.train()\n",
                "    epoch_loss = 0\n",
                "    \n",
                "    for src, trg in tqdm(dataloader, desc=\"Training\"):\n",
                "        src, trg = src.to(device), trg.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        # Forward pass\n",
                "        output = model(src, trg, teacher_forcing_ratio)\n",
                "        \n",
                "        # output: (batch_size, trg_len, vocab_size)\n",
                "        # trg: (batch_size, trg_len)\n",
                "        \n",
                "        # Reshape for loss calculation (ignore <SOS> token)\n",
                "        output = output[:, 1:, :].contiguous().view(-1, output.shape[-1])\n",
                "        trg = trg[:, 1:].contiguous().view(-1)\n",
                "        \n",
                "        # Calculate loss\n",
                "        loss = criterion(output, trg)\n",
                "        \n",
                "        # Backward pass\n",
                "        loss.backward()\n",
                "        \n",
                "        # Clip gradients\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
                "        \n",
                "        # Update weights\n",
                "        optimizer.step()\n",
                "        \n",
                "        epoch_loss += loss.item()\n",
                "    \n",
                "    return epoch_loss / len(dataloader)\n",
                "\n",
                "def evaluate(model, dataloader, criterion):\n",
                "    \"\"\"Evaluate the model\"\"\"\n",
                "    model.eval()\n",
                "    epoch_loss = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for src, trg in tqdm(dataloader, desc=\"Evaluating\"):\n",
                "            src, trg = src.to(device), trg.to(device)\n",
                "            \n",
                "            # Forward pass (no teacher forcing during evaluation)\n",
                "            output = model(src, trg, teacher_forcing_ratio=0)\n",
                "            \n",
                "            # Reshape for loss calculation\n",
                "            output = output[:, 1:, :].contiguous().view(-1, output.shape[-1])\n",
                "            trg = trg[:, 1:].contiguous().view(-1)\n",
                "            \n",
                "            # Calculate loss\n",
                "            loss = criterion(output, trg)\n",
                "            \n",
                "            epoch_loss += loss.item()\n",
                "    \n",
                "    return epoch_loss / len(dataloader)\n",
                "\n",
                "def plot_history(train_losses, val_losses):\n",
                "    \"\"\"Plot training history\"\"\"\n",
                "    plt.figure(figsize=(10, 6))\n",
                "    plt.plot(train_losses, label='Train Loss', marker='o')\n",
                "    plt.plot(val_losses, label='Validation Loss', marker='s')\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.ylabel('Loss')\n",
                "    plt.title('Seq2Seq with Attention - Training Progress')\n",
                "    plt.legend()\n",
                "    plt.grid(True)\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting training...\n",
                        "\n",
                        "Epoch 1/10\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training:   0%|          | 2/625 [00:49<4:19:10, 24.96s/it]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTEACHER_FORCING_RATIO\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m     24\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n",
                        "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, criterion, optimizer, clip, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# output: (batch_size, trg_len, vocab_size)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# trg: (batch_size, trg_len)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Reshape for loss calculation (ignore <SOS> token)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m output \u001b[38;5;241m=\u001b[39m output[:, \u001b[38;5;241m1\u001b[39m:, :]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
                        "File \u001b[1;32md:\\repos\\NLP_sections\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[1;32md:\\repos\\NLP_sections\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "Cell \u001b[1;32mIn[9], line 37\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[1;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Decode one token at a time\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, trg_len):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Forward through decoder\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     output, hidden, cell, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Store output\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     outputs[:, t, :] \u001b[38;5;241m=\u001b[39m output\n",
                        "File \u001b[1;32md:\\repos\\NLP_sections\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[1;32md:\\repos\\NLP_sections\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# Training hyperparameters\n",
                "EPOCHS = 10\n",
                "LEARNING_RATE = 0.001\n",
                "CLIP = 1\n",
                "TEACHER_FORCING_RATIO = 0.5\n",
                "\n",
                "# Loss and optimizer\n",
                "criterion = nn.CrossEntropyLoss(ignore_index=fr_vocab.word2index[PAD_TOKEN])\n",
                "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "\n",
                "# Training history\n",
                "train_losses = []\n",
                "val_losses = []\n",
                "\n",
                "print(\"Starting training...\\n\")\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
                "    \n",
                "    # Train\n",
                "    train_loss = train_epoch(model, train_loader, criterion, optimizer, CLIP, TEACHER_FORCING_RATIO)\n",
                "    \n",
                "    # Evaluate\n",
                "    val_loss = evaluate(model, val_loader, criterion)\n",
                "    \n",
                "    # Save history\n",
                "    train_losses.append(train_loss)\n",
                "    val_losses.append(val_loss)\n",
                "    \n",
                "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\\n\")\n",
                "\n",
                "# Plot training history\n",
                "plot_history(train_losses, val_losses)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
