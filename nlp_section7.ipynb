{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import re\n",
                "import csv\n",
                "from collections import Counter\n",
                "import torch.nn.functional as F"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "original_rnn_md",
            "metadata": {},
            "source": [
                "# Original Simple RNN Implementation\n",
                "This is the basic RNN implementation we started with, for reference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "original_rnn",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Original RNN Class\n",
                "class RNN(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size, output_size):\n",
                "        super(RNN, self).__init__()\n",
                "        self.hidden_size = hidden_size\n",
                "        \n",
                "        # input to hidden (i2h) weights \n",
                "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
                "        # hidden to output (h2o) weights\n",
                "        self.h2o = nn.Linear(input_size + hidden_size, output_size)\n",
                "\n",
                "    def forward(self, input, hidden):\n",
                "        # input: (batch_size, input_size)\n",
                "        # hidden: (batch_size, hidden_size)\n",
                "        # why combine? \n",
                "        combined = torch.cat((input, hidden), 1)\n",
                "        hidden = self.i2h(combined)\n",
                "        output = self.h2o(combined)\n",
                "        return output, hidden\n",
                "\n",
                "    def init_hidden(self, batch_size):\n",
                "        return torch.zeros(batch_size, self.hidden_size)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "educational_demo_md",
            "metadata": {},
            "source": [
                "# Educational Demo: Sentiment Analysis Step-by-Step\n",
                "We demonstrate how the RNN processes a sentence for sentiment classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "educational_demo",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Step-by-Step Forward Pass:\n",
                        "--------------------------------------------------\n",
                        "Word: this            | Hidden (first 3): [ 0.20034426 -0.08665498  0.11625531] ...\n",
                        "Logits: [[-0.12855901 -0.14238012]] \n",
                        "Word: movie           | Hidden (first 3): [ 0.20630811 -0.01775336  0.12262754] ...\n",
                        "Logits: [[-0.08699305 -0.04649505]] \n",
                        "Word: was             | Hidden (first 3): [ 0.28001308  0.17004642 -0.19548813] ...\n",
                        "Logits: [[-0.05168187  0.08595321]] \n",
                        "Word: absolutely      | Hidden (first 3): [ 0.43885353  0.04597702 -0.03268138] ...\n",
                        "Logits: [[-0.07313797 -0.12640046]] \n",
                        "Word: amazing         | Hidden (first 3): [ 0.49027127  0.19018902 -0.01289213] ...\n",
                        "Logits: [[-0.39773023  0.29555368]] \n",
                        "Word: and             | Hidden (first 3): [ 0.7256163  -0.11263353 -0.06685778] ...\n",
                        "Logits: [[-0.12968136 -0.01403646]] \n",
                        "Word: wonderful       | Hidden (first 3): [0.53845525 0.22406152 0.20921092] ...\n",
                        "Logits: [[-0.4114157   0.29998404]] \n",
                        "--------------------------------------------------\n",
                        "Final Output Logits: [[-0.4114157   0.29998404]]\n",
                        "Loss: 1.1108\n",
                        "--------------------------------------------------\n",
                        "Backpropagation complete.\n",
                        "Gradient norm for i2h weights: 0.8054\n",
                        "Optimization step complete.\n"
                    ]
                }
            ],
            "source": [
                "# 1. Data Preparation\n",
                "sentence = \"this movie was absolutely amazing and wonderful\"\n",
                "words = sentence.split()\n",
                "vocab_demo = list(set(words))\n",
                "word_to_ix_demo = {word: i for i, word in enumerate(vocab_demo)}\n",
                "input_size = len(vocab_demo)\n",
                "hidden_size = 10\n",
                "output_size = 2 # Positive/Negative\n",
                "\n",
                "def make_one_hot(word, word_to_ix):\n",
                "    vec = torch.zeros(1, input_size)\n",
                "    vec[0][word_to_ix[word]] = 1\n",
                "    return vec\n",
                "\n",
                "# 2. Model Initialization\n",
                "rnn_demo = RNN(input_size, hidden_size, output_size)\n",
                "hidden_demo = rnn_demo.init_hidden(1) # Batch size 1\n",
                "\n",
                "# 3. Step-by-Step Forward Pass\n",
                "print(\"Step-by-Step Forward Pass:\")\n",
                "print(\"-\" * 50)\n",
                "for word in words:\n",
                "    input_tensor = make_one_hot(word, word_to_ix_demo)\n",
                "    output_demo, hidden_demo = rnn_demo(input_tensor, hidden_demo)\n",
                "    print(f\"Word: {word:<15} | Hidden (first 3): {hidden_demo[0][:3].detach().numpy()} ...\")\n",
                "    print(f\"Logits: {output_demo.detach().numpy()} \")\n",
                "\n",
                "# 4. Loss Calculation\n",
                "target_class = torch.tensor([0], dtype=torch.long) # 0 = Positive (assuming)\n",
                "criterion_demo = nn.CrossEntropyLoss()\n",
                "loss_demo = criterion_demo(output_demo, target_class)\n",
                "print(\"-\" * 50)\n",
                "print(f\"Final Output Logits: {output_demo.detach().numpy()}\")\n",
                "print(f\"Loss: {loss_demo.item():.4f}\")\n",
                "\n",
                "# 5. Backpropagation\n",
                "rnn_demo.zero_grad()\n",
                "loss_demo.backward()\n",
                "print(\"-\" * 50)\n",
                "print(\"Backpropagation complete.\")\n",
                "print(f\"Gradient norm for i2h weights: {rnn_demo.i2h.weight.grad.norm().item():.4f}\")\n",
                "\n",
                "# 6. Optimization\n",
                "optimizer_demo = optim.SGD(rnn_demo.parameters(), lr=0.1)\n",
                "optimizer_demo.step()\n",
                "print(\"Optimization step complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data_loading_md",
            "metadata": {},
            "source": [
                "# 1. Load IMDB Dataset\n",
                "We load a subset of the IMDB dataset for sentiment classification (Positive vs Negative)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "data_loading",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded 2000 reviews.\n",
                        "Sample Review: One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. The...\n",
                        "Sample Label: 1\n",
                        "Vocabulary size: 5001\n"
                    ]
                }
            ],
            "source": [
                "reviews = []\n",
                "labels = []\n",
                "\n",
                "with open('data/imdb_subset.csv', 'r', encoding='utf-8') as f:\n",
                "    reader = csv.reader(f)\n",
                "    next(reader) # Skip header\n",
                "    for row in reader:\n",
                "        if len(row) >= 2:\n",
                "            reviews.append(row[0])\n",
                "            # Label: positive -> 1, negative -> 0\n",
                "            labels.append(1 if row[1].strip().lower() == 'positive' else 0)\n",
                "\n",
                "print(f\"Loaded {len(reviews)} reviews.\")\n",
                "print(f\"Sample Review: {reviews[0][:100]}...\")\n",
                "print(f\"Sample Label: {labels[0]}\")\n",
                "\n",
                "# Preprocessing\n",
                "def preprocess(text):\n",
                "    text = text.lower()\n",
                "    text = re.sub(r'[^a-z\\s]', '', text)\n",
                "    return text.split()\n",
                "\n",
                "tokenized_reviews = [preprocess(r) for r in reviews]\n",
                "\n",
                "# Build Vocabulary\n",
                "all_words = [word for review in tokenized_reviews for word in review]\n",
                "vocab_count = Counter(all_words)\n",
                "vocab = sorted(vocab_count, key=vocab_count.get, reverse=True)\n",
                "# Limit vocab size for speed\n",
                "vocab = vocab[:5000]\n",
                "word_to_ix = {word: i+1 for i, word in enumerate(vocab)} # 0 is padding\n",
                "ix_to_word = {i+1: word for i, word in enumerate(vocab)}\n",
                "ix_to_word[0] = '<PAD>'\n",
                "vocab_size = len(word_to_ix) + 1\n",
                "\n",
                "print(f\"Vocabulary size: {vocab_size}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "glove_md",
            "metadata": {},
            "source": [
                "# 2. Load GloVe Embeddings\n",
                "We use pretrained GloVe embeddings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "load_glove",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found embeddings for 4897 / 5000 words.\n"
                    ]
                }
            ],
            "source": [
                "def load_glove_embeddings(file_path, word_to_ix, embedding_dim=50):\n",
                "    embeddings = np.zeros((len(word_to_ix) + 1, embedding_dim))\n",
                "    found = 0\n",
                "    with open(file_path, 'r', encoding='utf-8') as f:\n",
                "        for line in f:\n",
                "            values = line.split()\n",
                "            word = values[0]\n",
                "            if word in word_to_ix:\n",
                "                vector = np.asarray(values[1:], dtype='float32')\n",
                "                embeddings[word_to_ix[word]] = vector\n",
                "                found += 1\n",
                "    print(f\"Found embeddings for {found} / {len(word_to_ix)} words.\")\n",
                "    return torch.tensor(embeddings, dtype=torch.float32)\n",
                "\n",
                "embedding_dim = 50\n",
                "pretrained_embeddings = load_glove_embeddings('data/glove.6B.50d.txt', word_to_ix, embedding_dim)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dataset_prep_md",
            "metadata": {},
            "source": [
                "# 3. Prepare Dataset (Padding)\n",
                "We pad sequences to a fixed length."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "dataset_prep",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "X shape: torch.Size([2000, 100])\n",
                        "y shape: torch.Size([2000])\n"
                    ]
                }
            ],
            "source": [
                "seq_length = 100 # Truncate/Pad to 100 words\n",
                "dataX = []\n",
                "dataY = labels\n",
                "\n",
                "for review in tokenized_reviews:\n",
                "    # Convert to indices\n",
                "    idxs = [word_to_ix.get(w, 0) for w in review if w in word_to_ix]\n",
                "    # Pad or Truncate\n",
                "    if len(idxs) < seq_length:\n",
                "        idxs = idxs + [0] * (seq_length - len(idxs))\n",
                "    else:\n",
                "        idxs = idxs[:seq_length]\n",
                "    dataX.append(idxs)\n",
                "\n",
                "X = torch.tensor(dataX, dtype=torch.long)\n",
                "y = torch.tensor(dataY, dtype=torch.long)\n",
                "\n",
                "print(f\"X shape: {X.shape}\")\n",
                "print(f\"y shape: {y.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "3f89fa76",
            "metadata": {},
            "outputs": [],
            "source": [
                "# divide into train and test\n",
                "# change torch random seed\n",
                "torch.manual_seed(1234)\n",
                "\n",
                "train_size = int(len(reviews) * 0.8)\n",
                "# shuffle\n",
                "perm = torch.randperm(len(reviews))\n",
                "X = X[perm]\n",
                "y = y[perm]\n",
                "X_train = X[:train_size]\n",
                "y_train = y[:train_size]\n",
                "X_test = X[train_size:]\n",
                "y_test = y[train_size:]"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rnn_model_md",
            "metadata": {},
            "source": [
                "# 4. Unrolled RNN Classifier\n",
                "We use the UnrolledRNN for classification (Many-to-One)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "rnn_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "class UnrolledRNN(nn.Module):\n",
                "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, num_layers=1, dropout_prob=0.5, pretrained_embeddings=None):\n",
                "        super(UnrolledRNN, self).__init__()\n",
                "        self.hidden_size = hidden_size\n",
                "        self.num_layers = num_layers\n",
                "        \n",
                "        if pretrained_embeddings is not None:\n",
                "            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False, padding_idx=0)\n",
                "        else:\n",
                "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
                "        \n",
                "        # self.dropout = nn.Dropout(dropout_prob)\n",
                "        \n",
                "        self.rnn_cells = nn.ModuleList()\n",
                "        for i in range(num_layers):\n",
                "            input_dim = embedding_dim if i == 0 else hidden_size\n",
                "            self.rnn_cells.append(nn.ModuleDict({\n",
                "                'i2h': nn.Linear(input_dim, hidden_size),\n",
                "                'h2h': nn.Linear(hidden_size, hidden_size)\n",
                "            }))\n",
                "            \n",
                "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
                "        \n",
                "    def forward(self, x, hidden, return_all_outputs=False):\n",
                "        # x: (batch_size, seq_length)\n",
                "        batch_size = x.size(0)\n",
                "        seq_length = x.size(1)\n",
                "        \n",
                "        embedded = self.embedding(x)\n",
                "        #embedded = self.dropout(embedded)\n",
                "        all_outputs = []\n",
                "        current_hidden = hidden\n",
                "        \n",
                "        for t in range(seq_length):\n",
                "            input_t = embedded[:, t, :]\n",
                "            next_hidden_states = []\n",
                "            \n",
                "            for layer_idx in range(self.num_layers):\n",
                "                cell = self.rnn_cells[layer_idx]\n",
                "                h_prev = current_hidden[layer_idx]\n",
                "                \n",
                "                layer_input = input_t if layer_idx == 0 else next_hidden_states[-1]\n",
                "                \n",
                "                h_t = torch.tanh(cell['i2h'](layer_input) + cell['h2h'](h_prev))\n",
                "                \n",
                "                # if layer_idx < self.num_layers - 1:\n",
                "                #     h_t = self.dropout(h_t)\n",
                "                    \n",
                "                next_hidden_states.append(h_t)\n",
                "            \n",
                "            current_hidden = torch.stack(next_hidden_states)\n",
                "            # We only care about the final output for classification, \n",
                "            # but we compute it at every step for the demo if needed.\n",
                "            output_t = self.fc_out(current_hidden[-1])\n",
                "            all_outputs.append(output_t)\n",
                "            \n",
                "        if return_all_outputs:\n",
                "            return torch.stack(all_outputs, dim=1), current_hidden\n",
                "        else:\n",
                "            # Return only the last output (batch_size, output_size)\n",
                "            return all_outputs[-1], current_hidden\n",
                "\n",
                "    def init_hidden(self, batch_size):\n",
                "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training_md",
            "metadata": {},
            "source": [
                "# 5. Training Loop\n",
                "We train the RNN on the IMDB subset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "id": "training",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting training...\n",
                        "Epoch 1/5, Loss: 0.5553, Acc: 51.50%\n",
                        "Epoch 2/5, Loss: 0.5353, Acc: 58.38%\n",
                        "Epoch 3/5, Loss: 0.5017, Acc: 65.31%\n",
                        "Epoch 4/5, Loss: 0.4471, Acc: 70.19%\n",
                        "Epoch 5/5, Loss: 0.3728, Acc: 76.50%\n"
                    ]
                }
            ],
            "source": [
                "hidden_size = 64\n",
                "learning_rate = 0.001\n",
                "epochs = 5\n",
                "batch_size = 32\n",
                "num_layers = 2\n",
                "output_size = 2 # Positive/Negative\n",
                "\n",
                "model = UnrolledRNN(vocab_size, embedding_dim, hidden_size, output_size, num_layers=num_layers, pretrained_embeddings=pretrained_embeddings)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
                "\n",
                "print(\"Starting training...\")\n",
                "for epoch in range(epochs):\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    # Shuffle\n",
                "    perm = torch.randperm(len(X_train))\n",
                "    X_shuffled = X_train[perm]\n",
                "    y_shuffled = y_train[perm]\n",
                "    \n",
                "    for i in range(0, len(X_train), batch_size):\n",
                "        inputs = X_shuffled[i:i+batch_size]\n",
                "        targets = y_shuffled[i:i+batch_size]\n",
                "        \n",
                "        if len(inputs) != batch_size: continue\n",
                "        \n",
                "        hidden = model.init_hidden(batch_size)\n",
                "        model.zero_grad()\n",
                "        \n",
                "        output, hidden = model(inputs, hidden)\n",
                "        \n",
                "        loss = criterion(output, targets)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        _, predicted = torch.max(output.data, 1)\n",
                "        total += targets.size(0)\n",
                "        correct += (predicted == targets).sum().item()\n",
                "        \n",
                "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / (len(X)/batch_size):.4f}, Acc: {100 * correct / total:.2f}%\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "test_evaluation",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test Loss: 0.9648\n",
                        "Test Accuracy: 52.25%\n"
                    ]
                }
            ],
            "source": [
                "# Evaluate on Test Set\n",
                "model.eval()\n",
                "correct = 0\n",
                "total = 0\n",
                "test_loss = 0\n",
                "\n",
                "with torch.no_grad():\n",
                "    for i in range(0, len(X_test), batch_size):\n",
                "        inputs = X_test[i:i+batch_size]\n",
                "        targets = y_test[i:i+batch_size]\n",
                "        \n",
                "        if len(inputs) == 0: continue\n",
                "\n",
                "        hidden = model.init_hidden(len(inputs))\n",
                "        output, _ = model(inputs, hidden)\n",
                "        loss = criterion(output, targets)\n",
                "        test_loss += loss.item()\n",
                "        \n",
                "        _, predicted = torch.max(output.data, 1)\n",
                "        total += targets.size(0)\n",
                "        correct += (predicted == targets).sum().item()\n",
                "\n",
                "print(f\"Test Loss: {test_loss / (len(X_test)/batch_size):.4f}\")\n",
                "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "inference_md",
            "metadata": {},
            "source": [
                "# 6. Inference\n",
                "Test the model on a new review."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "id": "inference",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Review: 'This movie was fantastic and I loved it!'\n",
                        "Prediction: Negative (Confidence: 0.5489)\n",
                        "Gradient Norm: 1.4757\n",
                        "Review: 'bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad bad '\n",
                        "Prediction: Negative (Confidence: 0.8313)\n",
                        "Gradient Norm: 1.2388\n"
                    ]
                }
            ],
            "source": [
                "def predict_sentiment(model, review, word_to_ix):\n",
                "    # Enable gradient tracking\n",
                "    model.eval()\n",
                "    words = preprocess(review)\n",
                "    idxs = [word_to_ix.get(w, 0) for w in words]\n",
                "    # Pad/Truncate\n",
                "    if len(idxs) < seq_length:\n",
                "        idxs = idxs + [0] * (seq_length - len(idxs))\n",
                "    else:\n",
                "        idxs = idxs[:seq_length]\n",
                "        \n",
                "    input_tensor = torch.tensor([idxs], dtype=torch.long)\n",
                "    hidden = model.init_hidden(1)\n",
                "    \n",
                "    model.zero_grad()\n",
                "    output, _ = model(input_tensor, hidden)\n",
                "    probs = F.softmax(output, dim=1)\n",
                "    top_prob, top_ix = torch.max(probs, 1)\n",
                "    \n",
                "    # Compute gradients based on the prediction to see the norm\n",
                "    loss = nn.CrossEntropyLoss()(output, top_ix)\n",
                "    loss.backward()\n",
                "    grad_norm = model.rnn_cells[0]['h2h'].weight.grad.norm().item()\n",
                "    \n",
                "    sentiment = \"Positive\" if top_ix.item() == 1 else \"Negative\"\n",
                "    print(f\"Review: '{review}'\")\n",
                "    print(f\"Prediction: {sentiment} (Confidence: {top_prob.item():.4f})\")\n",
                "    print(f\"Gradient Norm: {grad_norm:.4f}\")\n",
                "\n",
                "predict_sentiment(model, \"This movie was fantastic and I loved it!\", word_to_ix)\n",
                "predict_sentiment(model, \"bad \" * 1000, word_to_ix)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "gradient_clipping_md",
            "metadata": {},
            "source": [
                "# 8. Exploding Gradients and Clipping Fix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "gradient_clipping",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Gradient Norm (Before Clipping): 1679682306048.0000\n",
                        "Gradient Norm (After Clipping): 0.2699\n"
                    ]
                }
            ],
            "source": [
                "exploding_model = UnrolledRNN(vocab_size, embedding_dim, hidden_size, output_size, num_layers=1, pretrained_embeddings=pretrained_embeddings)\n",
                "\n",
                "with torch.no_grad():\n",
                "    exploding_model.rnn_cells[0]['h2h'].weight.data.normal_(0.0, 2.0)\n",
                "seq_len = 50\n",
                "input_seq = torch.randint(1, vocab_size, (1, seq_len))\n",
                "target = torch.tensor([0], dtype=torch.long)\n",
                "hidden = exploding_model.init_hidden(1)\n",
                "exploding_model.zero_grad()\n",
                "output, _ = exploding_model(input_seq, hidden)\n",
                "loss = criterion(output, target)\n",
                "loss.backward()\n",
                "grad_norm = exploding_model.rnn_cells[0]['h2h'].weight.grad.norm().item()\n",
                "print(f\"Gradient Norm (Before Clipping): {grad_norm:.4f}\")\n",
                "\n",
                "max_norm = 5.0\n",
                "torch.nn.utils.clip_grad_norm_(exploding_model.parameters(), max_norm)\n",
                "\n",
                "grad_norm_clipped = exploding_model.rnn_cells[0]['h2h'].weight.grad.norm().item()\n",
                "print(f\"Gradient Norm (After Clipping): {grad_norm_clipped:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "vanishing_gradients_md",
            "metadata": {},
            "source": [
                "# 8. Vanishing Gradients Demo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "vanishing_gradients",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "First Token Gradient (Long Sequence): 0.000000\n",
                        "First Token Gradient (Short Sequence): 0.042213\n"
                    ]
                }
            ],
            "source": [
                "vanishing_model = UnrolledRNN(vocab_size, embedding_dim, hidden_size, output_size, num_layers=1, pretrained_embeddings=pretrained_embeddings)\n",
                "\n",
                "with torch.no_grad():\n",
                "    # initialize with small weights to cause vanishing gradients\n",
                "    vanishing_model.rnn_cells[0]['h2h'].weight.data.normal_(0.0, 0.1)\n",
                "\n",
                "# long sequence\n",
                "long_input = torch.randint(1, vocab_size, (1, 100))\n",
                "target = torch.tensor([0], dtype=torch.long)\n",
                "\n",
                "hidden = vanishing_model.init_hidden(1)\n",
                "vanishing_model.zero_grad()\n",
                "output, _ = vanishing_model(long_input, hidden)\n",
                "loss = criterion(output, target)\n",
                "loss.backward()\n",
                "\n",
                "# get gradient of embedding for first token\n",
                "first_token_grad = vanishing_model.embedding.weight.grad[long_input[0, 0]].norm().item()\n",
                "print(f\"First Token Gradient (Long Sequence): {first_token_grad:.6f}\")\n",
                "\n",
                "# Short sequence\n",
                "vanishing_model.zero_grad()\n",
                "short_input = torch.randint(1, vocab_size, (1, 4))\n",
                "output, _ = vanishing_model(short_input, hidden)\n",
                "loss = criterion(output, target)\n",
                "loss.backward()\n",
                "\n",
                "first_token_grad_short = vanishing_model.embedding.weight.grad[short_input[0, 0]].norm().item()\n",
                "print(f\"First Token Gradient (Short Sequence): {first_token_grad_short:.6f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "12dfe205",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
