{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a513111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0a7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding adds information about the position of tokens in a sequence.\n",
    "    \n",
    "    Since Transformers don't have recurrence or convolution, they need explicit \n",
    "    position information. This is done using sine and cosine functions of different \n",
    "    frequencies.\n",
    "    \n",
    "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \n",
    "    where:\n",
    "        - pos is the position in the sequence\n",
    "        - i is the dimension index\n",
    "        - d_model is the embedding dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of the model (embedding size)\n",
    "            max_len: Maximum sequence length\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create a matrix of shape (max_len, d_model) for positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # Create position indices [0, 1, 2, ..., max_len-1]\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create the division term for the positional encoding formula\n",
    "        # This creates: [1, 10000^(2/d_model), 10000^(4/d_model), ...]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices in the array (2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices in the array (2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add a batch dimension: (max_len, d_model) -> (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but should be saved with the model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input embeddings of shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Embeddings with positional encoding added\n",
    "        \"\"\"\n",
    "        # Add positional encoding to input embeddings\n",
    "        # x.size(1) is the sequence length\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9192762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention allows the model to jointly attend to information from \n",
    "    different representation subspaces at different positions.\n",
    "    \n",
    "    The attention mechanism consists of:\n",
    "    1. Linear projections of queries, keys, and values\n",
    "    2. Scaled dot-product attention\n",
    "    3. Concatenation of attention outputs from multiple heads\n",
    "    4. Final linear projection\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) * V\n",
    "    \n",
    "    where:\n",
    "        - Q: Queries matrix\n",
    "        - K: Keys matrix\n",
    "        - V: Values matrix\n",
    "        - d_k: Dimension of keys\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of the model\n",
    "            num_heads: Number of attention heads\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension of each head\n",
    "        \n",
    "        # Linear layers for Q, K, V projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Final linear layer after concatenating heads\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Compute scaled dot-product attention.\n",
    "        \n",
    "        Args:\n",
    "            Q: Queries of shape (batch_size, num_heads, seq_len, d_k)\n",
    "            K: Keys of shape (batch_size, num_heads, seq_len, d_k)\n",
    "            V: Values of shape (batch_size, num_heads, seq_len, d_k)\n",
    "            mask: Optional mask of shape (batch_size, 1, seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output of shape (batch_size, num_heads, seq_len, d_k)\n",
    "            attention_weights: Attention weights of shape (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # Calculate attention scores: Q @ K^T / sqrt(d_k)\n",
    "        # Shape: (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask (if provided) by setting masked positions to large negative value\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        # Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Query tensor of shape (batch_size, seq_len, d_model)\n",
    "            key: Key tensor of shape (batch_size, seq_len, d_model)\n",
    "            value: Value tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional mask tensor\n",
    "        \n",
    "        Returns:\n",
    "            output: Multi-head attention output\n",
    "            attention_weights: Attention weights for visualization\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1. Linear projections in batch: (batch_size, seq_len, d_model)\n",
    "        Q = self.W_q(query)\n",
    "        K = self.W_k(key)\n",
    "        V = self.W_v(value)\n",
    "        \n",
    "        # 2. Split into multiple heads: (batch_size, num_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 3. Apply scaled dot-product attention\n",
    "        attn_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # 4. Concatenate heads: (batch_size, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "        \n",
    "        # 5. Apply final linear layer\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15afee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network consists of two linear transformations\n",
    "    with a ReLU activation in between.\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "    \n",
    "    This is applied to each position separately and identically. The same\n",
    "    FFN is applied at every position.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of the model\n",
    "            d_ff: Dimension of the feed-forward layer (typically 4 * d_model)\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        \n",
    "        # First linear layer expands dimension\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        \n",
    "        # Second linear layer projects back to d_model\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Apply first linear layer with ReLU activation\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Apply second linear layer\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39627f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer Encoder Layer consists of:\n",
    "    1. Multi-Head Self-Attention\n",
    "    2. Add & Norm (Residual connection + Layer Normalization)\n",
    "    3. Position-wise Feed-Forward Network\n",
    "    4. Add & Norm (Residual connection + Layer Normalization)\n",
    "    \n",
    "    Each sub-layer has a residual connection followed by layer normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of the model\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Dimension of feed-forward layer\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Position-wise feed-forward network\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 1. Multi-head self-attention with residual connection and layer norm\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout1(attn_output)  # Residual connection\n",
    "        x = self.norm1(x)  # Layer normalization\n",
    "        \n",
    "        # 2. Feed-forward network with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout2(ff_output)  # Residual connection\n",
    "        x = self.norm2(x)  # Layer normalization\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7dca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer Decoder Layer consists of:\n",
    "    1. Masked Multi-Head Self-Attention\n",
    "    2. Add & Norm\n",
    "    3. Multi-Head Cross-Attention (attending to encoder output)\n",
    "    4. Add & Norm\n",
    "    5. Position-wise Feed-Forward Network\n",
    "    6. Add & Norm\n",
    "    \n",
    "    The self-attention in the decoder is masked to prevent positions from \n",
    "    attending to subsequent positions (autoregressive property).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of the model\n",
    "            num_heads: Number of attention heads\n",
    "            d_ff: Dimension of feed-forward layer\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Masked multi-head self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Multi-head cross-attention (decoder attends to encoder)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Position-wise feed-forward network\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Decoder input of shape (batch_size, tgt_seq_len, d_model)\n",
    "            encoder_output: Encoder output of shape (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Source attention mask\n",
    "            tgt_mask: Target attention mask (for masking future positions)\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch_size, tgt_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 1. Masked multi-head self-attention with residual and layer norm\n",
    "        self_attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = x + self.dropout1(self_attn_output)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # 2. Multi-head cross-attention with residual and layer norm\n",
    "        # Query from decoder, Key and Value from encoder\n",
    "        cross_attn_output, _ = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
    "        x = x + self.dropout2(cross_attn_output)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # 3. Feed-forward network with residual and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        x = self.norm3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04fa2fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Transformer Encoder consists of:\n",
    "    1. Input Embedding + Positional Encoding\n",
    "    2. N stacked Encoder Layers\n",
    "    \n",
    "    The encoder processes the source sequence and produces a continuous \n",
    "    representation that the decoder can attend to.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, \n",
    "                 max_len=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary\n",
    "            d_model: Dimension of the model\n",
    "            num_heads: Number of attention heads\n",
    "            num_layers: Number of encoder layers\n",
    "            d_ff: Dimension of feed-forward layer\n",
    "            max_len: Maximum sequence length\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Stack of N encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src, src_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Source sequence of shape (batch_size, src_seq_len)\n",
    "            src_mask: Optional source mask\n",
    "        \n",
    "        Returns:\n",
    "            Encoder output of shape (batch_size, src_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # 1. Embedding and scaling\n",
    "        # The embeddings are scaled by sqrt(d_model) as per the paper\n",
    "        x = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # 2. Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # 3. Pass through each encoder layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c49bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Transformer Decoder consists of:\n",
    "    1. Output Embedding + Positional Encoding\n",
    "    2. N stacked Decoder Layers\n",
    "    3. Final linear layer to project to vocabulary\n",
    "    \n",
    "    The decoder generates the target sequence one token at a time,\n",
    "    using both the encoder output and previously generated tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff,\n",
    "                 max_len=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary\n",
    "            d_model: Dimension of the model\n",
    "            num_heads: Number of attention heads\n",
    "            num_layers: Number of decoder layers\n",
    "            d_ff: Dimension of feed-forward layer\n",
    "            max_len: Maximum sequence length\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Stack of N decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final linear layer to project to vocabulary\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, tgt, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Target sequence of shape (batch_size, tgt_seq_len)\n",
    "            encoder_output: Encoder output of shape (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Optional source mask\n",
    "            tgt_mask: Optional target mask (for masking future positions)\n",
    "        \n",
    "        Returns:\n",
    "            Decoder output of shape (batch_size, tgt_seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # 1. Embedding and scaling\n",
    "        x = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # 2. Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # 3. Pass through each decoder layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # 4. Project to vocabulary size\n",
    "        output = self.fc_out(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04163e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Transformer Decoder consists of:\n",
    "    1. Output Embedding + Positional Encoding\n",
    "    2. N stacked Decoder Layers\n",
    "    3. Final linear layer to project to vocabulary\n",
    "    \n",
    "    The decoder generates the target sequence one token at a time,\n",
    "    using both the encoder output and previously generated tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff,\n",
    "                 max_len=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of the vocabulary\n",
    "            d_model: Dimension of the model\n",
    "            num_heads: Number of attention heads\n",
    "            num_layers: Number of decoder layers\n",
    "            d_ff: Dimension of feed-forward layer\n",
    "            max_len: Maximum sequence length\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Stack of N decoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final linear layer to project to vocabulary\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, tgt, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: Target sequence of shape (batch_size, tgt_seq_len)\n",
    "            encoder_output: Encoder output of shape (batch_size, src_seq_len, d_model)\n",
    "            src_mask: Optional source mask\n",
    "            tgt_mask: Optional target mask (for masking future positions)\n",
    "        \n",
    "        Returns:\n",
    "            Decoder output of shape (batch_size, tgt_seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        # 1. Embedding and scaling\n",
    "        x = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # 2. Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # 3. Pass through each decoder layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # 4. Project to vocabulary size\n",
    "        output = self.fc_out(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. COMPLETE TRANSFORMER MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer model combining Encoder and Decoder.\n",
    "    \n",
    "    The Transformer is used for sequence-to-sequence tasks like machine translation,\n",
    "    text summarization, etc.\n",
    "    \n",
    "    Architecture:\n",
    "        Source -> Encoder -> Encoder Output\n",
    "        Target -> Decoder (with Encoder Output) -> Output Logits\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n",
    "                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048,\n",
    "                 max_len=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_vocab_size: Source vocabulary size\n",
    "            tgt_vocab_size: Target vocabulary size\n",
    "            d_model: Dimension of the model (default: 512)\n",
    "            num_heads: Number of attention heads (default: 8)\n",
    "            num_encoder_layers: Number of encoder layers (default: 6)\n",
    "            num_decoder_layers: Number of decoder layers (default: 6)\n",
    "            d_ff: Dimension of feed-forward layer (default: 2048)\n",
    "            max_len: Maximum sequence length (default: 5000)\n",
    "            dropout: Dropout probability (default: 0.1)\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = TransformerEncoder(\n",
    "            src_vocab_size, d_model, num_heads, num_encoder_layers,\n",
    "            d_ff, max_len, dropout\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = TransformerDecoder(\n",
    "            tgt_vocab_size, d_model, num_heads, num_decoder_layers,\n",
    "            d_ff, max_len, dropout\n",
    "        )\n",
    "    \n",
    "    def make_src_mask(self, src, src_pad_idx=0):\n",
    "        \"\"\"\n",
    "        Create mask for source sequence to ignore padding tokens.\n",
    "        \n",
    "        Args:\n",
    "            src: Source sequence of shape (batch_size, src_seq_len)\n",
    "            src_pad_idx: Index of padding token\n",
    "        \n",
    "        Returns:\n",
    "            Source mask of shape (batch_size, 1, 1, src_seq_len)\n",
    "        \"\"\"\n",
    "        src_mask = (src != src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_tgt_mask(self, tgt, tgt_pad_idx=0):\n",
    "        \"\"\"\n",
    "        Create mask for target sequence to:\n",
    "        1. Ignore padding tokens\n",
    "        2. Prevent attention to future positions (causal mask)\n",
    "        \n",
    "        Args:\n",
    "            tgt: Target sequence of shape (batch_size, tgt_seq_len)\n",
    "            tgt_pad_idx: Index of padding token\n",
    "        \n",
    "        Returns:\n",
    "            Target mask of shape (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        \n",
    "        # Create padding mask\n",
    "        tgt_pad_mask = (tgt != tgt_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # Shape: (batch_size, 1, 1, tgt_seq_len)\n",
    "        \n",
    "        # Create subsequent mask (no look ahead mask)\n",
    "        tgt_sub_mask = torch.tril(\n",
    "            torch.ones((tgt_len, tgt_len), device=tgt.device)\n",
    "        ).bool()\n",
    "        # Shape: (tgt_seq_len, tgt_seq_len)\n",
    "        \n",
    "        # Combine both masks\n",
    "        tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
    "        # Shape: (batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "        \n",
    "        return tgt_mask\n",
    "    \n",
    "    def forward(self, src, tgt, src_pad_idx=0, tgt_pad_idx=0):\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer.\n",
    "        \n",
    "        Args:\n",
    "            src: Source sequence of shape (batch_size, src_seq_len)\n",
    "            tgt: Target sequence of shape (batch_size, tgt_seq_len)\n",
    "            src_pad_idx: Source padding token index\n",
    "            tgt_pad_idx: Target padding token index\n",
    "        \n",
    "        Returns:\n",
    "            Output logits of shape (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        # Create masks\n",
    "        src_mask = self.make_src_mask(src, src_pad_idx)\n",
    "        tgt_mask = self.make_tgt_mask(tgt, tgt_pad_idx)\n",
    "        \n",
    "        # Encode source sequence\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        \n",
    "        # Decode to generate target sequence\n",
    "        output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "565175d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_transformer():\n",
    "    \"\"\"\n",
    "    Create a sample Transformer model with common hyperparameters.\n",
    "    \n",
    "    This example uses parameters similar to the base model in the paper:\n",
    "    - d_model = 512\n",
    "    - num_heads = 8\n",
    "    - num_layers = 6\n",
    "    - d_ff = 2048\n",
    "    \"\"\"\n",
    "    # Example vocabulary sizes\n",
    "    src_vocab_size = 10000  # Source language vocabulary\n",
    "    tgt_vocab_size = 10000  # Target language vocabulary\n",
    "    \n",
    "    # Create the model\n",
    "    model = Transformer(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_encoder_layers=6,\n",
    "        num_decoder_layers=6,\n",
    "        d_ff=2048,\n",
    "        max_len=100,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2d83594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (embedding): Embedding(10000, 512)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionWiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (embedding): Embedding(10000, 512)\n",
       "    (pos_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderLayer(\n",
       "        (self_attn): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionWiseFeedForward(\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=512, out_features=10000, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_sample_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bbe2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
